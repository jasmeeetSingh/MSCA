{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c866c2-6d78-409c-a6be-1e2914172c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wikipedia\n",
    "\n",
    "# The NLTK stopwords had a limited number of stopwords so I used SpaCy for better results\n",
    "!pip install spacy\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "\n",
    "!pip uninstall nltk -y\n",
    "!pip install nltk\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)\n",
    "print(\"Wikipedia, SpaCy installation Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694391b0-bbf7-4fc7-af12-80b30686a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import re\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import LancasterStemmer, SnowballStemmer, PorterStemmer\n",
    "\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(language = 'english')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b585aed-1ddd-4b92-a9b0-184f653e5bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting names of all places in the world so that we can remove them from the final corpus of the wikipedia pages\n",
    "\n",
    "!pip install geosky\n",
    "from geosky import geo_plug\n",
    "\n",
    "import json\n",
    "city_names = json.loads(geo_plug.all_State_CityNames())\n",
    "\n",
    "geography = []\n",
    "final_places_country_list = []\n",
    "for i in geo_plug.all_CountryNames():\n",
    "    final_places_country_list.append(i.lower())\n",
    "    \n",
    "for i in city_names:\n",
    "    for j in (list(i.keys())):\n",
    "        final_places_country_list.append(j.lower())\n",
    "    for j in list(i.values()):\n",
    "        for k in j:\n",
    "            final_places_country_list.append(k.lower())\n",
    "            \n",
    "clear_output(wait = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f265e92-64f2-4b61-9ec8-46919e5349aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing stopwords\n",
    "all_stopwords = sp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c687daae-f5bb-4588-bc4b-b87b337d31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_page_content(title_of_page):\n",
    "    p = wikipedia.page(title = title_of_page, auto_suggest = False)\n",
    "    return p.content # Content of page.\n",
    "\n",
    "def get_lowercase_corpus(content):\n",
    "    corpus = content.split('\\n')\n",
    "    corpus_lower = []\n",
    "    for i in corpus:\n",
    "        corpus_lower.append(re.sub(r'[^a-z0-9\\-\\ ]', '', i.lower()))\n",
    "        \n",
    "    return corpus_lower\n",
    "\n",
    "def get_main_corpus_removing_stopwords(corpus_lower):\n",
    "    main_corpus = []\n",
    "    for i in corpus_lower:\n",
    "        if(len(i) > 10):\n",
    "            stri = ''\n",
    "            for j in i.split():\n",
    "                if (j not in all_stopwords and len(j) > 0 and j not in final_places_country_list):\n",
    "                    stri += (j + ' ')\n",
    "            main_corpus.append(stri.strip())\n",
    "            \n",
    "    return main_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f9c99c-af6f-4724-8558-9f0755301667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter_words(main_corpus):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf.fit_transform(main_corpus)\n",
    "    # get idf values\n",
    "    res = []\n",
    "    for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n",
    "        if(ele1.isnumeric() == False):\n",
    "            res.append([ele1, ele2])\n",
    "    return [word[0] for word in sorted(res,key=lambda l:l[1], reverse = False) if len(word[0]) > 4][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1944cc-2bfd-41e2-a4c3-34f5aae591f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_words_from_wikipedia_page(page_title):\n",
    "    content = wikipedia_page_content(page_title)\n",
    "    corpus_lower = get_lowercase_corpus(content)\n",
    "    main_corpus = get_main_corpus_removing_stopwords(corpus_lower)\n",
    "    return list(set(get_filter_words(main_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4245c6cf-edd6-40f3-8208-7300fe37c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output(wait = True)\n",
    "print(\"Loaded Wikipedia Word Extractor file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a6cfe0-ab24-4c4d-8439-ced4a1aa7f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages = ['K-12', 'K-12_education_in_the_United_States','No_Child_Left_Behind_Act']\n",
    "\n",
    "# keywords = []\n",
    "# for i in pages:\n",
    "#     words = get_key_words_from_wikipedia_page(i)\n",
    "#     for j in words:\n",
    "#         keywords.append(j)\n",
    "        \n",
    "# print(list(set(keywords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92823ce9-7ee9-4b73-921e-be918939adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lancaster.stem('schools'))\n",
    "# print(lancaster.stem('school'))\n",
    "# print(lancaster.stem('american'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae40f636-a078-43c3-901f-b80f2303a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(snowball.stem('schools'))\n",
    "# print(snowball.stem('school'))\n",
    "# print(snowball.stem('secondary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdace6-3fd7-450a-ba3a-b41ffdb62a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ps.stem('schools'))\n",
    "# print(ps.stem('school'))\n",
    "# print(ps.stem('american'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
